{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8583790,"sourceType":"datasetVersion","datasetId":5012123}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install google-auth google-auth-oauthlib google-auth-httplib2 pydrive\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms as T\nfrom torch.utils.data.dataloader import default_collate\nfrom PIL import Image\nimport json\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_IMG_FOLDER_PATH = '/kaggle/input/icpr-vistac-dataset-for-object-detection/Combined-VISTAC-Challenge-Dataset/Combined-VISTAC-Challenge-Dataset/train' #Training image path.\nVALID_IMG_FOLDER_PATH = '/kaggle/input/icpr-vistac-dataset-for-object-detection/Combined-VISTAC-Challenge-Dataset/Combined-VISTAC-Challenge-Dataset/validation' # Valid image path for checking.\nTRAIN_ANNOTATION_FILE = '/kaggle/input/icpr-vistac-dataset-for-object-detection/train.json'\nVAL_ANNOTATION_FILE = '/kaggle/input/icpr-vistac-dataset-for-object-detection/validation.json'\nLR = 0.001 #LEARNING RATE\nBATCH_SIZE = 8\nEPOCHS = 10\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODEL_NAME = 'object_detection_using_faster_RCNN'\nprint(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# Load the JSON files\nwith open(TRAIN_ANNOTATION_FILE, 'r') as f:\n    train_annotations = json.load(f)\n\nwith open(VAL_ANNOTATION_FILE, 'r') as f:\n    validation_annotations = json.load(f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data transformations\ntrain_augs = T.Compose([\n          T.RandomHorizontalFlip(p = 0.5),\n          T.RandomRotation(degrees=(-20, +20)),\n          T.ToTensor()\n          #PIL/numpy array -> torch tensor -> (height, width, channel) -> (channel, height, width)\n    ])\nvalid_augs = T.Compose([\n    T.ToTensor()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load annotations from a JSON file\ndef load_annotations(file_path):\n    with open(file_path, 'r') as f:\n        annotations = json.load(f)\n    return annotations\n\nclass VistaDataset(Dataset):\n    def __init__(self, img_folder, annotations, transforms=None):\n        self.img_folder = img_folder\n        self.annotations = annotations\n        self.transforms = transforms\n        self.imgs = []\n        for video_dir, data in annotations.items():\n            for img_name in data['img_names']:\n                self.imgs.append((video_dir, img_name))\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        video_dir, img_name = self.imgs[idx]\n        img_path = os.path.join(self.img_folder, img_name)\n\n        # Debugging print statements\n        #print(f\"video_dir: {video_dir}\")\n        #print(f\"img_name: {img_name}\")\n        #print(f\"img_path: {img_path}\")\n\n        # Check if the file exists before opening it\n        if not os.path.exists(img_path):\n            print(f\"File not found: {img_path}\")\n            raise FileNotFoundError(f\"File not found: {img_path}\")\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        # Find the corresponding ground truth box\n        boxes = []\n        labels = []\n\n        for i, name in enumerate(self.annotations[video_dir]['img_names']):\n            if name == img_name:\n                box = self.annotations[video_dir]['gt_rect'][i]\n                boxes.append([box[0], box[1], box[0] + box[2], box[1] + box[3]])\n                labels.append(1)  # Assuming label 1 for now; adjust according to class mapping\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n\n        if self.transforms:\n            img = self.transforms(img)\n\n        return img, target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load annotations\ntrain_annotations = load_annotations(TRAIN_ANNOTATION_FILE)\nvalid_annotations = load_annotations(VAL_ANNOTATION_FILE)\n\ntrainset = VistaDataset(TRAIN_IMG_FOLDER_PATH, train_annotations, transforms=train_augs)\nvalidset = VistaDataset(VALID_IMG_FOLDER_PATH, valid_annotations, transforms=valid_augs)\n\nprint(f\"Total no. of examples in trainset : {len(trainset)}\")\nprint(f\"Total no. of examples in validset : {len(validset)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nvalidloader = DataLoader(validset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\nprint(f\"Total no. of batches in trainloader : {len(trainloader)}\")\nprint(f\"Total no. of batches in validloader : {len(validloader)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\ndef get_instance_segmentation_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n# Initialize the model and load it to the device\nmodel = get_instance_segmentation_model(num_classes=71)\nmodel.to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=LR, momentum=0.9, weight_decay=0.0005)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the function to train the model\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    running_loss = 0.0\n    for i, (images, targets) in enumerate(data_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        running_loss += losses.item()\n        if i % print_freq == 0:\n            print(f\"Epoch [{epoch}/{EPOCHS}], Step [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\")\n\n    print(f\"Epoch [{epoch}] Loss: {running_loss/len(data_loader):.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    running_loss = 0.0\n    num_batches = 0\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            try:\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                running_loss += losses.item()\n                num_batches += 1\n            except Exception as e:\n                logger.error(f\"Error during evaluation: {e}\")\n                continue\n\n    if num_batches > 0:\n        avg_loss = running_loss / num_batches\n        print(f\"Validation Loss: {avg_loss:.4f}\")\n    else:\n        print(\"No loss computed during evaluation.\")\n\n    return avg_loss if num_batches > 0 else None\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.patches as patches\n\ndef visualize_results(image, results):\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(image)\n\n    for box, label in zip(results['boxes'].detach().numpy(), results['labels']):\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='green', facecolor=\"none\")\n        ax.add_patch(rect)\n        plt.text(x1, y1, label, bbox=dict(facecolor='white', alpha=0.5), fontsize=12)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    train_one_epoch(model, optimizer, trainloader, device=DEVICE, epoch=epoch, print_freq=50)\n    eval_loss = evaluate(model, validloader, device=DEVICE)\n    \n    # Save the model after each epoch\n    model_save_path = f'/kaggle/working/faster_rcnn_model_epoch_{epoch}.pth'\n    torch.save(model.state_dict(), model_save_path)\n    \n    if eval_loss is None:\n        print(f\"No valid evaluation loss for epoch {epoch}, please check the validation process.\")\n    else:\n        print(f\"Epoch [{epoch}] Evaluation Loss: {eval_loss:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the Model\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\ndef get_instance_segmentation_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\ndef load_model(model_path, num_classes):\n    model = get_instance_segmentation_model(num_classes=num_classes)\n    model.load_state_dict(torch.load(model_path))\n    model.to(DEVICE)\n    model.eval()  # Set the model to evaluation mode\n    return model\n\nmodel_path = '/kaggle/working/faster_rcnn_model.pth'\nloaded_model = load_model(model_path, num_classes=71)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef visualize_results(image, predictions):\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(image)\n\n    for box, label, score in zip(predictions['boxes'].cpu().numpy(), predictions['labels'].cpu().numpy(), predictions['scores'].cpu().numpy()):\n        if score > 0.5:  # Adjust the threshold as needed\n            x1, y1, x2, y2 = box\n            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='green', facecolor=\"none\")\n            ax.add_patch(rect)\n            plt.text(x1, y1, f\"{label}: {score:.2f}\", bbox=dict(facecolor='white', alpha=0.5), fontsize=12)\n\n    plt.show()\n\n# Run inference and visualize results for a few samples\nfor images, targets in validloader:\n    images = list(image.to(DEVICE) for image in images)\n    \n    with torch.no_grad():\n        predictions = loaded_model(images)\n\n    for i, image in enumerate(images):\n        image_np = image.permute(1, 2, 0).cpu().numpy()  # Convert tensor to numpy array for plotting\n        visualize_results(image_np, predictions[i])\n\n    break  # Remove this to run on the entire validation set\n","metadata":{},"execution_count":null,"outputs":[]}]}